# data/csv_to_npz.py

from loguru import logger
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Optional
import sys
import time
import talib
import math
from pathlib import Path
import arcticdb as adb  # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º ArcticDB

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–≥–µ—Ä–∞ Loguru —Å —ç–º–æ–¥–∑–∏ –∏ —É–ª—É—á—à–µ–Ω–Ω—ã–º —Ü–≤–µ—Ç–æ–≤—ã–º –æ—Ñ–æ—Ä–º–ª–µ–Ω–∏–µ–º
log_dir = "logs"
logger.remove()  # –£–¥–∞–ª—è–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –æ–±—Ä–∞–±–æ—Ç—á–∏–∫
logger.add(
    sys.stderr,
    format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level: <8}</level> | <cyan>{function}</cyan>:<yellow>{line}</yellow> - <level>{message}</level>",
    level="INFO",
    colorize=True
)
logger.add(
    f"{log_dir}/arcticdb_to_npz_preprocessing_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log",
    rotation="10 MB",
    retention="30 days",
    format="{time:YYYY-MM-DD HH:mm:ss} | {level: <8} | {function}:{line} - {message}",
    level="DEBUG"
)


class FinancialDataPreprocessor:
    """–ö–ª–∞—Å—Å –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –∏–∑ ArcticDB –≤ —Ñ–æ—Ä–º–∞—Ç .npz –¥–ª—è ML-–º–æ–¥–µ–ª–µ–π"""

    def __init__(self, ticker: str = "DOGEUSDT", window_size: int = 150,
                 timeframe: str = "1m", market_type: str = "linear",
                 arctic_path: str = "arcticdb_storage",
                 library_name: str = "bybit_market_data",
                 days_back: float = 30.0):
        self.ticker = ticker
        self.window_size = window_size
        self.timeframe = timeframe
        self.market_type = market_type
        self.arctic_path = arctic_path
        self.library_name = library_name
        self.days_back = days_back
        self.symbol_name = f"{ticker}_{timeframe}_{market_type}"

        # –ë–∞–∑–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ –±—É–¥—É—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω—ã —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–º–∏ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–∞–º–∏
        self.feature_columns = ["open", "high", "low", "close", "volume"]
        self.logger = logger.bind(component="FinancialPreprocessor")

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è ArcticDB
        self._init_arcticdb()

    def _init_arcticdb(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ ArcticDB"""
        try:
            self.logger.info(f"üîß –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è ArcticDB —Ö—Ä–∞–Ω–∏–ª–∏—â–∞: {self.arctic_path}")
            self.ac = adb.Arctic(f"lmdb://{self.arctic_path}")

            if not self.ac.has_library(self.library_name):
                self.logger.error(f"‚ùå –ë–∏–±–ª–∏–æ—Ç–µ–∫–∞ '{self.library_name}' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ ArcticDB")
                raise ValueError(f"–ë–∏–±–ª–∏–æ—Ç–µ–∫–∞ '{self.library_name}' –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç")

            self.library = self.ac.get_library(self.library_name)
            self.logger.success(f"üóÑÔ∏è –£—Å–ø–µ—à–Ω–æ –ø–æ–¥–∫–ª—é—á–µ–Ω–æ –∫ –±–∏–±–ª–∏–æ—Ç–µ–∫–µ: {self.library_name}")
        except Exception as e:
            self.logger.exception(f"üî• –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ ArcticDB: {str(e)}")
            raise


    @logger.catch
    def validate_data(self, df: pd.DataFrame) -> None:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç–∏ –¥–∞–Ω–Ω—ã—Ö —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ–± –æ—à–∏–±–∫–∞—Ö"""
        self.logger.debug("üîç –ù–∞—á–∞–ª–æ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö")

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫
        required_columns = ["date"] + self.feature_columns
        missing = [col for col in required_columns if col not in df.columns]
        if missing:
            self.logger.error(f"‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –∫–æ–ª–æ–Ω–∫–∏: {missing}")
            raise ValueError(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –∫–æ–ª–æ–Ω–∫–∏: {missing}")

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–æ—Ä–º–∞—Ç–∞ –¥–∞—Ç—ã
        try:
            df["date"] = pd.to_datetime(df["date"])
            self.logger.debug("üìÖ –î–∞—Ç–∞ —É—Å–ø–µ—à–Ω–æ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∞ –≤ datetime —Ñ–æ—Ä–º–∞—Ç")
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –¥–∞—Ç—ã: {str(e)}")
            raise ValueError("–ö–æ–ª–æ–Ω–∫–∞ 'date' –∏–º–µ–µ—Ç –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç") from e

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –Ω–∞–ª–∏—á–∏–µ –ø—Ä–æ–ø—É—Å–∫–æ–≤ (–ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ NaN)
        null_counts = df[self.feature_columns].isnull().sum()
        total_null = null_counts.sum()

        if total_null > 0:
            self.logger.warning(f"‚ö†Ô∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ {total_null} –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π")
            for col, count in null_counts.items():
                if count > 0:
                    self.logger.debug(f"üîç –ü—Ä–æ–ø—É—Å–∫–∏ –≤ '{col}': {count}")
        else:
            self.logger.success("‚úÖ –ù–µ—Ç –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –≤ –¥–∞–Ω–Ω—ã—Ö")

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –ø–æ—Ä—è–¥–∫–∞
        if not df['date'].is_monotonic_increasing:
            self.logger.warning("‚ö†Ô∏è –î–∞–Ω–Ω—ã–µ –Ω–µ –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã –ø–æ –≤–æ–∑—Ä–∞—Å—Ç–∞–Ω–∏—é –¥–∞—Ç—ã")
            df = df.sort_values('date').reset_index(drop=True)
            self.logger.info("üîÑ –î–∞–Ω–Ω—ã–µ –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã –ø–æ –¥–∞—Ç–µ")

        self.logger.success("‚úÖ –í–∞–ª–∏–¥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")

    @logger.catch
    def create_windows(self, df: pd.DataFrame, dataset_type: str) -> Tuple[
        List[np.ndarray], Dict[str, Tuple[str, datetime]]]:
        """–°–æ–∑–¥–∞–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫–æ–Ω –∏–∑ –¥–∞–Ω–Ω—ã—Ö"""
        self.logger.info(f"üîÑ –°–æ–∑–¥–∞–Ω–∏–µ –æ–∫–æ–Ω –¥–ª—è {dataset_type} (—Ä–∞–∑–º–µ—Ä –æ–∫–Ω–∞: {self.window_size})")

        windows = []
        keys_map = {}
        total_rows = len(df)

        if total_rows < self.window_size:
            self.logger.error(
                f"‚ùå –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –æ–∫–æ–Ω (—Ç—Ä–µ–±—É–µ—Ç—Å—è {self.window_size}, –¥–æ—Å—Ç—É–ø–Ω–æ {total_rows})")
            return [], {}

        # –ü—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä –¥–ª—è –±–æ–ª—å—à–∏—Ö –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö
        start_time = time.time()
        progress_interval = max(1, (total_rows - self.window_size) // 10)

        for i in range(total_rows - self.window_size + 1):
            if i % progress_interval == 0:
                progress = (i + 1) / (total_rows - self.window_size + 1) * 100
                self.logger.debug(
                    f"‚è≥ –ü—Ä–æ–≥—Ä–µ—Å—Å —Å–æ–∑–¥–∞–Ω–∏—è –æ–∫–æ–Ω: {progress:.1f}% ({i + 1}/{total_rows - self.window_size + 1})")

            window_data = df.iloc[i:i + self.window_size][self.feature_columns].values
            last_date = df.iloc[i + self.window_size - 1]["date"]

            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–∫–Ω–∞ –∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–µ–≥–æ –∫–ª—é—á–∞
            windows.append(window_data)
            keys_map[str(len(windows) - 1)] = (self.ticker, last_date)

        elapsed = time.time() - start_time
        self.logger.success(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ {len(windows)} –æ–∫–æ–Ω –∑–∞ {elapsed:.2f} —Å–µ–∫—É–Ω–¥ üöÄ")
        return windows, keys_map

    @logger.catch
    def save_npz(self, windows: List[np.ndarray], keys_map: Dict, output_path: str) -> None:
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –≤ —Ñ–æ—Ä–º–∞—Ç–µ .npz"""
        self.logger.info(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ {len(windows)} –æ–∫–æ–Ω –≤ {output_path}")

        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞–Ω–Ω—ã—Ö
        arrays = {str(i): window for i, window in enumerate(windows)}
        arrays["_keys_map_"] = keys_map

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–æ–≤
        if windows:
            sample = windows[0]
            self.logger.debug(f"üìê –§–æ—Ä–º–∞—Ç –¥–∞–Ω–Ω—ã—Ö: {sample.shape} (–ø—Ä–∏–º–µ—Ä –¥–ª—è –æ–∫–Ω–∞ 0)")

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å –∏–∑–º–µ—Ä–µ–Ω–∏–µ–º –≤—Ä–µ–º–µ–Ω–∏
        start_time = time.time()
        np.savez_compressed(output_path, **arrays)
        elapsed = time.time() - start_time

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–∞ —Ñ–∞–π–ª–∞
        file_size = Path(output_path).stat().st_size / (1024 * 1024)
        self.logger.success(f"‚úÖ –§–∞–π–ª —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {output_path} ({file_size:.2f} MB, {elapsed:.2f} —Å–µ–∫) üì¶")

    @logger.catch
    def load_data_from_arcticdb(self) -> pd.DataFrame:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ ArcticDB —Å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å—é —É–∫–∞–∑–∞–Ω–∏—è –≥–ª—É–±–∏–Ω—ã"""
        self.logger.info(f"üì• –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ ArcticDB –¥–ª—è {self.symbol_name}")
        self.logger.info(f"‚öôÔ∏è –ü–∞—Ä–∞–º–µ—Ç—Ä—ã: {self.days_back} –¥–Ω–µ–π, —Ç–∞–π–º—Ñ—Ä–µ–π–º {self.timeframe}")

        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –¥–∏–∞–ø–∞–∑–æ–Ω–∞
        end_date = datetime.now()
        start_date = end_date - timedelta(days=self.days_back)

        self.logger.info(f"üìÖ –ó–∞–ø—Ä–∞—à–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∑–∞ –ø–µ—Ä–∏–æ–¥: {start_date} - {end_date}")

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è —Å–∏–º–≤–æ–ª–∞
        if not self.library.has_symbol(self.symbol_name):
            self.logger.error(f"‚ùå –°–∏–º–≤–æ–ª '{self.symbol_name}' –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –±–∏–±–ª–∏–æ—Ç–µ–∫–µ '{self.library_name}'")
            raise ValueError(f"–°–∏–º–≤–æ–ª '{self.symbol_name}' –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –≤ –±–∏–±–ª–∏–æ—Ç–µ–∫–µ")

        # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π –ø–æ –≤—Ä–µ–º–µ–Ω–∏
        # –í ArcticDB 2.x –∏—Å–ø–æ–ª—å–∑—É–µ–º –∫–æ—Ä—Ç–µ–∂ (start, end) –≤–º–µ—Å—Ç–æ DateRange
        try:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤—Å–µ –¥–∞–Ω–Ω—ã–µ –±–µ–∑ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ —Å–Ω–∞—á–∞–ª–∞
            self.logger.debug("üîç –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ ArcticDB...")
            data = self.library.read(self.symbol_name).data

            # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ –≤—Ä–µ–º–µ–Ω–Ω–æ–º—É –¥–∏–∞–ø–∞–∑–æ–Ω—É
            if 'date' in data.columns:
                data = data[(data['date'] >= start_date) & (data['date'] <= end_date)]
            else:
                self.logger.error("‚ùå –í –¥–∞–Ω–Ω—ã—Ö –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –∫–æ–ª–æ–Ω–∫–∞ 'date'")
                raise ValueError("–î–∞–Ω–Ω—ã–µ –Ω–µ —Å–æ–¥–µ—Ä–∂–∞—Ç –∫–æ–ª–æ–Ω–∫—É 'date'")

            self.logger.success(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(data)} –∑–∞–ø–∏—Å–µ–π –∑–∞ {self.days_back} –¥–Ω–µ–π")

            # –ü—Ä–æ–≤–µ—Ä–∫–∞, –µ—Å—Ç—å –ª–∏ –¥–∞–Ω–Ω—ã–µ –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
            if len(data) == 0:
                self.logger.warning("‚ö†Ô∏è –ü–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã–µ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç")
                return pd.DataFrame()

            return data

        except Exception as e:
            self.logger.exception(f"üî• –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö –∏–∑ ArcticDB: {str(e)}")
            raise

    @logger.catch
    def process_dataset(self, output_files: Dict[str, str], percentages: Dict[str, float]) -> None:
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö –∏–∑ ArcticDB"""
        self.logger.info(f"üöÄ –ù–∞—á–∞–ª–æ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö –∏–∑ ArcticDB")
        start_time = time.time()

        try:
            # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ ArcticDB
            df = self.load_data_from_arcticdb()

            if df.empty:
                self.logger.error("‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø–æ—Å–ª–µ –∑–∞–≥—Ä—É–∑–∫–∏ –∏–∑ ArcticDB")
                return

            # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –∏ —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞
            self.logger.info("üîÑ –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –∏ —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö")
            df["date"] = pd.to_datetime(df["date"])
            df = df.sort_values("date").reset_index(drop=True)
            self.logger.info(f"üìÖ –û—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–æ {len(df)} –∑–∞–ø–∏—Å–µ–π —Å {df['date'].min()} –ø–æ {df['date'].max()}")

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è
            time_diff = df["date"].diff().min()
            self.logger.debug(f"‚è±Ô∏è –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –∏–Ω—Ç–µ—Ä–≤–∞–ª –¥–∞–Ω–Ω—ã—Ö: {time_diff}")

#            # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤ (–ü–û–°–õ–ï —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏!)
#            df = self.add_technical_indicators(df)

            # –í–∞–ª–∏–¥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
            self.validate_data(df)

            # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
            self.logger.info("‚úÇÔ∏è –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –Ω–∞ –Ω–∞–±–æ—Ä—ã")
            total = len(df)
            cumulative = 0
            splits = {}

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—É–º–º—ã –ø—Ä–æ—Ü–µ–Ω—Ç–æ–≤
            total_percent = sum(percentages.values())
            if abs(total_percent - 1.0) > 0.01:
                self.logger.warning(f"‚ö†Ô∏è –°—É–º–º–∞ –ø—Ä–æ—Ü–µ–Ω—Ç–æ–≤ ({total_percent:.2f}) –Ω–µ —Ä–∞–≤–Ω–∞ 1.0, –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∞")
                percentages = {k: v / total_percent for k, v in percentages.items()}

            for dataset, percent in percentages.items():
                if percent <= 0:
                    continue

                segment_size = int(total * percent)
                end_idx = min(cumulative + segment_size, total)
                splits[dataset] = (cumulative, end_idx)
                cumulative = end_idx
                self.logger.info(f"üéØ {dataset}: {percent:.1%} ({segment_size} –∑–∞–ø–∏—Å–µ–π) [{cumulative}/{total}]")

            # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞–∂–¥–æ–≥–æ –Ω–∞–±–æ—Ä–∞
            for dataset, (start_idx, end_idx) in splits.items():
                if dataset not in output_files:
                    continue

                self.logger.info(f"üîß –û–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–∞–±–æ—Ä–∞ '{dataset}'")
                segment = df.iloc[start_idx:end_idx].copy()

                # –°–æ–∑–¥–∞–Ω–∏–µ –æ–∫–æ–Ω
                windows, keys_map = self.create_windows(segment, dataset)

                # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
                if windows:
                    self.save_npz(windows, keys_map, output_files[dataset])
                else:
                    self.logger.warning(f"‚ö†Ô∏è –ü—Ä–æ–ø—É—Å–∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è {dataset}: –Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–∫–æ–Ω")

            total_time = time.time() - start_time
            self.logger.success(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ –∑–∞ {total_time:.2f} —Å–µ–∫—É–Ω–¥! üéØ")

        except Exception as e:
            self.logger.exception(f"üí• –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –¥–∞–Ω–Ω—ã—Ö: {str(e)}")
            raise


# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
if __name__ == "__main__":
    logger.info("üöÄ –ó–∞–ø—É—Å–∫ Financial Data Preprocessor –¥–ª—è ArcticDB")

    try:
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ ArcticDB
        ARCTIC_PATH = "arcticdb_storage"
        LIBRARY_NAME = "bybit_market_data"
        TICKER = "DOGEUSDT"
        TIMEFRAME = "1m"
        MARKET_TYPE = "linear"
        DAYS_BACK = 100.0  # –ì–ª—É–±–∏–Ω–∞ –∑–∞–ø—Ä–æ—Å–∞ –≤ –¥–Ω—è—Ö

        # –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–µ –ø—Ä–æ—Ü–µ–Ω—Ç—ã
        percentages = {
            "train": 0.75,
            "val": 0.05,
            "test": 0.10,
            "backtest": 0.10
        }

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—É–º–º—ã –ø—Ä–æ—Ü–µ–Ω—Ç–æ–≤
        total_percent = sum(percentages.values())
        if abs(total_percent - 1.0) > 0.01:
            logger.warning(f"‚ö†Ô∏è –°—É–º–º–∞ –ø—Ä–æ—Ü–µ–Ω—Ç–æ–≤ ({total_percent:.2f}) –Ω–µ —Ä–∞–≤–Ω–∞ 1.0")

        preprocessor = FinancialDataPreprocessor(
            ticker=TICKER,
            window_size=150,
            timeframe=TIMEFRAME,
            market_type=MARKET_TYPE,
            arctic_path=ARCTIC_PATH,
            library_name=LIBRARY_NAME,
            days_back=DAYS_BACK
        )

        preprocessor.process_dataset(
            output_files={
                "train": "train_data.npz",
                "val": "val_data.npz",
                "test": "test_data.npz",
                "backtest": "backtest_data.npz"
            },
            percentages=percentages
        )

        logger.success("üéâ –í—Å–µ —Ñ–∞–π–ª—ã —É—Å–ø–µ—à–Ω–æ —Å–æ–∑–¥–∞–Ω—ã —Å —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–º–∏ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–∞–º–∏!")

    except Exception as e:
        logger.critical(f"üí• –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã –∏–∑-–∑–∞ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–æ–π –æ—à–∏–±–∫–∏: {str(e)}")
        sys.exit(1)