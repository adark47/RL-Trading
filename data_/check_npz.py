# data/check_npz.py

from loguru import logger
import numpy as np
from pathlib import Path
import sys
import pandas as pd
from datetime import datetime
from tabulate import tabulate
from collections import defaultdict
from typing import Dict, List, Tuple, Optional

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–≥–µ—Ä–∞ Loguru —Å —ç–º–æ–¥–∑–∏ –∏ —É–ª—É—á—à–µ–Ω–Ω—ã–º —Ü–≤–µ—Ç–æ–≤—ã–º –æ—Ñ–æ—Ä–º–ª–µ–Ω–∏–µ–º
log_dir = "logs"
logger.remove()  # –£–¥–∞–ª—è–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –æ–±—Ä–∞–±–æ—Ç—á–∏–∫
logger.add(
    sys.stderr,
    format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level: <8}</level> | <cyan>{function}</cyan>:<yellow>{line}</yellow> - <level>{message}</level>",
    level="INFO",
    colorize=True
)
logger.add(
    f"{log_dir}/npz_validation_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log",
    rotation="10 MB",
    retention="30 days",
    format="{time:YYYY-MM-DD HH:mm:ss} | {level: <8} | {function}:{line} - {message}",
    level="DEBUG"
)


class NPZValidator:
    """–ö–ª–∞—Å—Å –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏ —Ñ–∞–π–ª–æ–≤ .npz, —Å–æ–¥–µ—Ä–∂–∞—â–∏—Ö —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–µ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ä—è–¥—ã –¥–ª—è ML"""

    def __init__(self, window_size: int = 150):
        self.window_size = window_size
        self.logger = logger.bind(component="NPZValidator")
        self.validation_results = defaultdict(dict)

    def _get_array_preview(self, array: np.ndarray, max_rows: int = 3) -> str:
        """–°–æ–∑–¥–∞–µ—Ç –ø—Ä–µ–≤—å—é –º–∞—Å—Å–∏–≤–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º tabulate"""
        if array.ndim == 1:
            preview_data = array[:max_rows].reshape(-1, 1)
            headers = ["Value"]
        else:
            preview_data = array[:max_rows, :]
            headers = [f"Feature {i}" for i in range(preview_data.shape[1])]

        return tabulate(preview_data,
                        headers=headers,
                        tablefmt="psql",
                        floatfmt=".4f",
                        showindex="always")

    def validate_file(self, file_path: str) -> Dict:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è –æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞ .npz"""
        file_path = Path(file_path)
        self.logger.info(f"üîç –ù–∞—á–∞–ª–æ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ —Ñ–∞–π–ª–∞: {file_path.name} üìÅ")
        result = {
            "exists": False,
            "valid_structure": False,
            "window_count": 0,
            "window_shape": None,
            "feature_count": 0,
            "contains_nan": False,
            "temporal_consistency": False,
            "key_map_valid": False,
            "date_range": None,
            "ticker": None
        }

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è —Ñ–∞–π–ª–∞
        if not file_path.exists():
            self.logger.error(f"‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {file_path}")
            return result

        result["exists"] = True
        self.logger.success(f"‚úÖ –§–∞–π–ª –Ω–∞–π–¥–µ–Ω: {file_path} ({file_path.stat().st_size / (1024 * 1024):.2f} MB) üì¶")

        try:
            # –ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–∞
            with np.load(file_path, allow_pickle=True) as data:
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
                all_keys = list(data.keys())
                array_keys = [k for k in all_keys if k != "_keys_map_"]
                has_key_map = "_keys_map_" in all_keys

                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –∫–ª—é—á–µ–≤–æ–π –∫–∞—Ä—Ç—ã
                if not has_key_map:
                    self.logger.error("‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–π –∫–ª—é—á '_keys_map_' –≤ —Ñ–∞–π–ª–µ")
                else:
                    result["key_map_valid"] = True
                    self.logger.info("‚úÖ –ù–∞–π–¥–µ–Ω –∫–ª—é—á '_keys_map_' —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏ üó∫Ô∏è")

                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –¥–∞–Ω–Ω—ã—Ö
                if not array_keys:
                    self.logger.error("‚ùå –§–∞–π–ª –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –¥–∞–Ω–Ω—ã—Ö (–æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç —á–∏—Å–ª–æ–≤—ã–µ –º–∞—Å—Å–∏–≤—ã)")
                    return result

                # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞–Ω–Ω—ã—Ö
                first_array = data[array_keys[0]]
                window_shape = first_array.shape
                result["window_shape"] = window_shape
                result["feature_count"] = window_shape[1] if len(window_shape) > 1 else 1
                result["window_count"] = len(array_keys)

                self.logger.info(f"üìä –ù–∞–π–¥–µ–Ω–æ {result['window_count']} –æ–∫–æ–Ω —Ä–∞–∑–º–µ—Ä–æ–º {window_shape}")

                # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–æ—Ä–º—ã –æ–∫–æ–Ω
                inconsistent_shapes = []
                for key in array_keys:
                    if data[key].shape != window_shape:
                        inconsistent_shapes.append((key, data[key].shape))

                if inconsistent_shapes:
                    self.logger.error(f"‚ùå –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –Ω–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –≤ —Ñ–æ—Ä–º–µ –æ–∫–æ–Ω: {inconsistent_shapes}")
                else:
                    result["valid_structure"] = True
                    self.logger.success(f"‚úÖ –í—Å–µ –æ–∫–Ω–∞ –∏–º–µ—é—Ç –æ–¥–∏–Ω–∞–∫–æ–≤—É—é —Ñ–æ—Ä–º—É: {window_shape} üìê")

                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ NaN
                contains_nan = False
                nan_count = 0
                for key in array_keys[:10]:  # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 10 –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
                    if np.isnan(data[key]).any():
                        contains_nan = True
                        nan_count += np.isnan(data[key]).sum()

                if contains_nan:
                    self.logger.warning(f"‚ö†Ô∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã NaN –∑–Ω–∞—á–µ–Ω–∏—è (–ø—Ä–∏–º–µ—Ä–Ω–æ {nan_count}+) –≤ –¥–∞–Ω–Ω—ã—Ö")
                    result["contains_nan"] = True
                else:
                    self.logger.success("‚úÖ –î–∞–Ω–Ω—ã–µ –Ω–µ —Å–æ–¥–µ—Ä–∂–∞—Ç NaN –∑–Ω–∞—á–µ–Ω–∏–π üßπ")

                # –ê–Ω–∞–ª–∏–∑ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–µ—Ç–æ–∫
                if has_key_map:
                    keys_map = data["_keys_map_"].item() if isinstance(data["_keys_map_"], np.ndarray) else data[
                        "_keys_map_"]
                    dates = []
                    tickers = set()

                    for idx, (ticker, date) in keys_map.items():
                        try:
                            # –ü—Ä–æ–±—É–µ–º –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å –≤ datetime, –µ—Å–ª–∏ —ç—Ç–æ —Å—Ç—Ä–æ–∫–∞
                            if isinstance(date, str):
                                date = pd.to_datetime(date)
                            dates.append(date)
                            tickers.add(ticker)
                        except Exception as e:
                            self.logger.debug(f"‚ö†Ô∏è –ü—Ä–æ–±–ª–µ–º–∞ —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –¥–∞—Ç—ã –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞ {idx}: {e}")

                    if dates:
                        dates = sorted(dates)
                        result["date_range"] = (dates[0], dates[-1])
                        result["ticker"] = tickers.pop() if len(tickers) == 1 else "Mixed"

                        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –º–æ–Ω–æ—Ç–æ–Ω–Ω–æ—Å—Ç–∏
                        is_monotonic = all(dates[i] <= dates[i + 1] for i in range(len(dates) - 1))
                        result["temporal_consistency"] = is_monotonic

                        if is_monotonic:
                            self.logger.success(f"‚úÖ –í—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç–∫–∏ –º–æ–Ω–æ—Ç–æ–Ω–Ω—ã: –æ—Ç {dates[0]} –¥–æ {dates[-1]} ‚è±Ô∏è")
                        else:
                            self.logger.error(
                                "‚ùå –í—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç–∫–∏ –ù–ï –º–æ–Ω–æ—Ç–æ–Ω–Ω—ã - –≤–æ–∑–º–æ–∂–Ω—ã –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏—è –∏–ª–∏ –Ω–∞—Ä—É—à–µ–Ω–∏–µ –ø–æ—Ä—è–¥–∫–∞")

                        # –ü—Ä–µ–≤—å—é –¥–∞—Ç
                        preview_dates = [(i, d) for i, d in list(enumerate(dates))[:3]] + \
                                        [(i, d) for i, d in list(enumerate(dates))[-3:]]
                        dates_table = tabulate(preview_dates,
                                               headers=["Window Index", "Date"],
                                               tablefmt="psql")
                        self.logger.info(f"üìÖ –ü—Ä–µ–≤—å—é –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–µ—Ç–æ–∫:\n{dates_table}")
                    else:
                        self.logger.warning("‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç–∫–∏ –∏–∑ _keys_map_")

                # –ü—Ä–µ–≤—å—é –¥–∞–Ω–Ω—ã—Ö
                if array_keys:
                    preview_key = array_keys[0]
                    preview_data = data[preview_key]
                    preview_text = self._get_array_preview(preview_data)
                    self.logger.info(f"üîç –ü—Ä–µ–≤—å—é –¥–∞–Ω–Ω—ã—Ö (–æ–∫–Ω–æ {preview_key}):\n{preview_text}")

                # –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑
                if array_keys:
                    sample = data[array_keys[0]]
                    if sample.ndim > 1:  # –ï—Å–ª–∏ —ç—Ç–æ –º–Ω–æ–≥–æ–º–µ—Ä–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
                        stats = {
                            "min": np.min(sample),
                            "max": np.max(sample),
                            "mean": np.mean(sample),
                            "std": np.std(sample)
                        }
                        stats_table = [
                            ["Min", f"{stats['min']:.4f}"],
                            ["Max", f"{stats['max']:.4f}"],
                            ["Mean", f"{stats['mean']:.4f}"],
                            ["Std", f"{stats['std']:.4f}"]
                        ]
                        self.logger.info(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –ø—Ä–∏–º–µ—Ä—É –¥–∞–Ω–Ω—ã—Ö:\n{tabulate(stats_table, tablefmt='psql')}")

        except Exception as e:
            self.logger.exception(f"üí• –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ —Ñ–∞–π–ª–∞: {str(e)}")
            return result

        self.logger.success(f"‚úÖ –í–∞–ª–∏–¥–∞—Ü–∏—è —Ñ–∞–π–ª–∞ {file_path.name} –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ! üéØ")
        return result

    def compare_datasets(self, dataset_results: Dict[str, Dict]) -> None:
        """–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–π –∏ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏"""
        self.logger.info("üîç –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç–∏ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è")

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–π –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–≤
        date_ranges = {}
        for dataset, result in dataset_results.items():
            if result["date_range"]:
                start, end = result["date_range"]
                date_ranges[dataset] = (start, end)
                self.logger.debug(f"üïí {dataset} –¥–∏–∞–ø–∞–∑–æ–Ω: {start} - {end}")

        if len(date_ranges) >= 2:
            sorted_datasets = sorted(date_ranges.items(), key=lambda x: x[1][0])

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–≤
            for i in range(1, len(sorted_datasets)):
                prev_dataset, (prev_start, prev_end) = sorted_datasets[i - 1]
                curr_dataset, (curr_start, curr_end) = sorted_datasets[i]

                if prev_end > curr_start:
                    self.logger.error(
                        f"‚ùå –ü–ï–†–ï–°–ï–ß–ï–ù–ò–ï: {prev_dataset} –∑–∞–∫–∞–Ω—á–∏–≤–∞–µ—Ç—Å—è {prev_end}, –Ω–æ {curr_dataset} –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è {curr_start}")
                else:
                    self.logger.success(f"‚úÖ –ù–µ—Ç –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏—è –º–µ–∂–¥—É {prev_dataset} –∏ {curr_dataset}")

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ —Ä–∞–∑–º–µ—Ä–æ–≤ –æ–∫–æ–Ω
        window_shapes = {ds: res["window_shape"] for ds, res in dataset_results.items() if res["window_shape"]}
        if len(set(window_shapes.values())) > 1:
            self.logger.warning(f"‚ö†Ô∏è –†–∞–∑–Ω—ã–µ —Ñ–æ—Ä–º—ã –æ–∫–æ–Ω –º–µ–∂–¥—É –Ω–∞–±–æ—Ä–∞–º–∏: {window_shapes}")
        elif window_shapes:
            self.logger.success(f"‚úÖ –í—Å–µ –Ω–∞–±–æ—Ä—ã –∏–º–µ—é—Ç –æ–¥–∏–Ω–∞–∫–æ–≤—É—é —Ñ–æ—Ä–º—É –æ–∫–æ–Ω: {list(window_shapes.values())[0]}")

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ñ–∏—á
        feature_counts = {ds: res["feature_count"] for ds, res in dataset_results.items()}
        if len(set(feature_counts.values())) > 1:
            self.logger.error(f"‚ùå –†–∞–∑–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–∏—á –º–µ–∂–¥—É –Ω–∞–±–æ—Ä–∞–º–∏: {feature_counts}")
        else:
            self.logger.success(f"‚úÖ –í—Å–µ –Ω–∞–±–æ—Ä—ã –∏–º–µ—é—Ç –æ–¥–∏–Ω–∞–∫–æ–≤–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–∏—á: {list(feature_counts.values())[0]}")

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö (—É–ø—Ä–æ—â–µ–Ω–Ω–∞—è)
        self.logger.info("üìä –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –º–µ–∂–¥—É –Ω–∞–±–æ—Ä–∞–º–∏")
        total_windows = sum(res["window_count"] for res in dataset_results.values())
        for ds, res in dataset_results.items():
            percent = res["window_count"] / total_windows * 100 if total_windows > 0 else 0
            self.logger.info(f"üìà {ds}: {res['window_count']} –æ–∫–æ–Ω ({percent:.1f}%)")

    def generate_validation_report(self, file_paths: List[str]) -> None:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–æ–ª–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞ –ø–æ –≤—Å–µ–º —Ñ–∞–π–ª–∞–º"""
        self.logger.info("üìù –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–æ–ª–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –¥–ª—è –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö")

        # –í–∞–ª–∏–¥–∞—Ü–∏—è –∫–∞–∂–¥–æ–≥–æ —Ñ–∞–π–ª–∞
        dataset_results = {}
        for file_path in file_paths:
            dataset_name = Path(file_path).stem.split('_')[0]  # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–º—è –Ω–∞–±–æ—Ä–∞ (train, val –∏ —Ç.–¥.)
            result = self.validate_file(file_path)
            dataset_results[dataset_name] = result
            self.validation_results[dataset_name] = result

        # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –Ω–∞–±–æ—Ä–æ–≤
        self.compare_datasets(dataset_results)

        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–≤–æ–¥–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞
        summary = []
        for dataset, result in dataset_results.items():
            status = "‚úÖ" if all([
                result["exists"],
                result["valid_structure"],
                not result["contains_nan"],
                result["temporal_consistency"],
                result["key_map_valid"]
            ]) else "‚ùå"

            summary.append([
                status,
                dataset,
                "–î–∞" if result["exists"] else "–ù–µ—Ç",
                result["window_count"],
                str(result["window_shape"]) if result["window_shape"] else "N/A",
                "–ù–µ—Ç" if not result["contains_nan"] else "–î–∞",
                f"{result['date_range'][0]} - {result['date_range'][1]}" if result["date_range"] else "N/A"
            ])

        # –í—ã–≤–æ–¥ —Å–≤–æ–¥–Ω–æ–π —Ç–∞–±–ª–∏—Ü—ã
        summary_table = tabulate(
            summary,
            headers=["–°—Ç–∞—Ç—É—Å", "–ù–∞–±–æ—Ä", "–°—É—â–µ—Å—Ç–≤—É–µ—Ç", "–ö–æ–ª-–≤–æ –æ–∫–æ–Ω", "–§–æ—Ä–º–∞ –æ–∫–Ω–∞", "–ï—Å—Ç—å NaN", "–î–∏–∞–ø–∞–∑–æ–Ω –¥–∞—Ç"],
            tablefmt="grid"
        )
        self.logger.info(f"\nüìã –°–≤–æ–¥–Ω—ã–π –æ—Ç—á–µ—Ç –≤–∞–ª–∏–¥–∞—Ü–∏–∏:\n{summary_table}")

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–±—â–∏—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
        self._check_best_practices(dataset_results)

        self.logger.success("‚úÖ –ü–æ–ª–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞! –í—Å–µ –ø—Ä–æ–≤–µ—Ä–∫–∏ –ø—Ä–æ–π–¥–µ–Ω—ã. üèÜ")

    def _check_best_practices(self, dataset_results: Dict[str, Dict]) -> None:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–±–ª—é–¥–µ–Ω–∏—è –ª—É—á—à–∏—Ö –ø—Ä–∞–∫—Ç–∏–∫ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        self.logger.info("üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–±–ª—é–¥–µ–Ω–∏—è –ª—É—á—à–∏—Ö –ø—Ä–∞–∫—Ç–∏–∫ –≤–∞–ª–∏–¥–∞—Ü–∏–∏")

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ—Å—Ç–∏ –¥–∞–Ω–Ω—ã—Ö
        total_windows = sum(res["window_count"] for res in dataset_results.values())
        if total_windows < 1000:
            self.logger.warning(
                "‚ö†Ô∏è –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–∫–æ–Ω –º–µ–Ω—å—à–µ 1000 - –º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–ª—è –Ω–∞–¥–µ–∂–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è ML –º–æ–¥–µ–ª–µ–π")
        else:
            self.logger.success("‚úÖ –î–æ—Å—Ç–∞—Ç–æ—á–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π")

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö
        required_datasets = ["train", "val", "test"]
        missing_datasets = [ds for ds in required_datasets if ds not in dataset_results]

        if missing_datasets:
            self.logger.error(f"‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –Ω–∞–±–æ—Ä—ã –¥–∞–Ω–Ω—ã—Ö: {missing_datasets}")
        else:
            self.logger.success("‚úÖ –í—Å–µ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –Ω–∞–±–æ—Ä—ã –¥–∞–Ω–Ω—ã—Ö –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É—é—Ç (train, val, test)")
            self.logger.info(
                "Data Validation: Verify AI outputs against traditional models during transition periods. [[7]]")

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–æ–≤ –Ω–∞–±–æ—Ä–æ–≤
        if "train" in dataset_results and "val" in dataset_results and "test" in dataset_results:
            train_size = dataset_results["train"]["window_count"]
            val_size = dataset_results["val"]["window_count"]
            test_size = dataset_results["test"]["window_count"]

            total = train_size + val_size + test_size
            if train_size / total < 0.6:
                self.logger.warning(f"‚ö†Ô∏è –î–æ–ª—è –æ–±—É—á–∞—é—â–µ–≥–æ –Ω–∞–±–æ—Ä–∞ ({train_size / total:.1%}) –Ω–∏–∂–µ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã—Ö 60-70%")
            if val_size / total < 0.1:
                self.logger.warning(f"‚ö†Ô∏è –î–æ–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ –Ω–∞–±–æ—Ä–∞ ({val_size / total:.1%}) –Ω–∏–∂–µ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã—Ö 10-20%")
            if test_size / total < 0.1:
                self.logger.warning(f"‚ö†Ô∏è –î–æ–ª—è —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –Ω–∞–±–æ—Ä–∞ ({test_size / total:.1%}) –Ω–∏–∂–µ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã—Ö 10-20%")
            else:
                self.logger.success("‚úÖ –°–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ –Ω–∞–±–æ—Ä–æ–≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –ª—É—á—à–∏–º –ø—Ä–∞–∫—Ç–∏–∫–∞–º")

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω–æ–π —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç–∏
        if len(dataset_results) >= 2:
            date_ranges = {ds: res["date_range"] for ds, res in dataset_results.items() if res["date_range"]}
            if len(date_ranges) == len(dataset_results):
                sorted_ranges = sorted(date_ranges.items(), key=lambda x: x[1][0] if x[1] else pd.Timestamp.min)

                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
                for i in range(1, len(sorted_ranges)):
                    prev_end = sorted_ranges[i - 1][1][1]
                    curr_start = sorted_ranges[i][1][0]
                    if prev_end >= curr_start:
                        self.logger.error(
                            f"‚ùå –ù–∞—Ä—É—à–µ–Ω–∞ –≤—Ä–µ–º–µ–Ω–Ω–∞—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º–µ–∂–¥—É {sorted_ranges[i - 1][0]} –∏ {sorted_ranges[i][0]}")
                    else:
                        self.logger.success(
                            f"‚úÖ –í—Ä–µ–º–µ–Ω–Ω–∞—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –º–µ–∂–¥—É {sorted_ranges[i - 1][0]} –∏ {sorted_ranges[i][0]}")

        self.logger.info(
            "Learn important data validation best practices and techniques to improve data integrity for financial planning, forecasting, and budgeting. [[3]]")
        self.logger.info(
            "When validating financial data, ensure temporal consistency to prevent look-ahead bias in models. [[6]]")


if __name__ == "__main__":
    logger.info("üîç –ó–∞–ø—É—Å–∫ –≤–∞–ª–∏–¥–∞—Ç–æ—Ä–∞ NPZ —Ñ–∞–π–ª–æ–≤ –¥–ª—è —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö")

    try:
        # –§–∞–π–ª—ã –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏
        files_to_validate = [
            "train_data.npz",
            "val_data.npz",
            "test_data.npz",
            "backtest_data.npz"
        ]

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è —Ñ–∞–π–ª–æ–≤ –ø–µ—Ä–µ–¥ –Ω–∞—á–∞–ª–æ–º
        existing_files = [f for f in files_to_validate if Path(f).exists()]
        missing_files = [f for f in files_to_validate if not Path(f).exists()]

        if missing_files:
            logger.warning(f"‚ö†Ô∏è –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç —Ñ–∞–π–ª—ã: {missing_files}")

        if not existing_files:
            logger.error("‚ùå –ù–µ—Ç —Ñ–∞–π–ª–æ–≤ –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø—É—Ç–∏ –∫ —Ñ–∞–π–ª–∞–º.")
            sys.exit(1)

        # –°–æ–∑–¥–∞–Ω–∏–µ –∏ –∑–∞–ø—É—Å–∫ –≤–∞–ª–∏–¥–∞—Ç–æ—Ä–∞
        validator = NPZValidator(window_size=150)
        validator.generate_validation_report(existing_files)

        logger.success("üéâ –í–∞–ª–∏–¥–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ! –í—Å–µ –ø—Ä–æ–≤–µ—Ä–∫–∏ –ø—Ä–æ–π–¥–µ–Ω—ã. üèÜ")

    except Exception as e:
        logger.exception(f"üí• –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –≤–æ –≤—Ä–µ–º—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {str(e)}")
        sys.exit(1)