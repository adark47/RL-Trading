# data/csv_to_npz.py

from loguru import logger
import pandas as pd
import numpy as np
from datetime import datetime
from typing import Dict, List, Tuple, Callable
import sys
import time
import talib  # –î–æ–±–∞–≤–ª–µ–Ω –∏–º–ø–æ—Ä—Ç TA-Lib
import math
from pathlib import Path
from tabulate import tabulate

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–≥–µ—Ä–∞ Loguru —Å —ç–º–æ–¥–∑–∏ –∏ —É–ª—É—á—à–µ–Ω–Ω—ã–º —Ü–≤–µ—Ç–æ–≤—ã–º –æ—Ñ–æ—Ä–º–ª–µ–Ω–∏–µ–º
log_dir = "logs"
logger.remove()  # –£–¥–∞–ª—è–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –æ–±—Ä–∞–±–æ—Ç—á–∏–∫
logger.add(
    sys.stderr,
    format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level: <8}</level> | <cyan>{function}</cyan>:<yellow>{line}</yellow> - <level>{message}</level>",
    level="INFO",
    colorize=True
)
logger.add(
    f"{log_dir}/csv_to_npz_preprocessing_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log",
    rotation="10 MB",
    retention="30 days",
    format="{time:YYYY-MM-DD HH:mm:ss} | {level: <8} | {function}:{line} - {message}",
    level="DEBUG"
)


class FinancialDataPreprocessor:
    """–ö–ª–∞—Å—Å –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è CSV —Å —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏ –≤ —Ñ–æ—Ä–º–∞—Ç .npz –¥–ª—è ML-–º–æ–¥–µ–ª–µ–π"""

    def __init__(self, ticker: str = "STOCK", window_size: int = 150):
        self.ticker = ticker
        self.window_size = window_size
        # –ë–∞–∑–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ –±—É–¥—É—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω—ã —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–º–∏ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–∞–º–∏
        self.feature_columns = ["open", "high", "low", "close", "volume"]
        self.logger = logger.bind(component="FinancialPreprocessor")

    def calculate_hma(self, close: pd.Series, timeperiod: int) -> pd.Series:
        """–†–∞—Å—á–µ—Ç Hull Moving Average —á–µ—Ä–µ–∑ WMA, —Ç–∞–∫ –∫–∞–∫ –ø—Ä—è–º–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è HMA –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –≤ TA-Lib"""
        self.logger.debug(f"–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º HMA —Å –ø–µ—Ä–∏–æ–¥–æ–º {timeperiod} üìê")

        # –§–æ—Ä–º—É–ª–∞ HMA: HMA = WMA(2*WMA(n/2) - WMA(n), sqrt(n))
        half_period = max(1, int(timeperiod / 2))
        sqrt_period = max(1, int(math.sqrt(timeperiod)))

        wma_half = talib.WMA(close, timeperiod=half_period)
        wma_full = talib.WMA(close, timeperiod=timeperiod)

        # –í—ã—á–∏—Å–ª—è–µ–º —Ä–∞–∑–Ω–∏—Ü—É —Å —É—á–µ—Ç–æ–º NaN –∑–Ω–∞—á–µ–Ω–∏–π
        diff = 2 * wma_half - wma_full
        hma = talib.WMA(diff, timeperiod=sqrt_period)

        return hma

    @logger.catch
    def add_technical_indicators(self, df: pd.DataFrame) -> pd.DataFrame:
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤ –≤ DataFrame"""
        self.logger.info("–î–æ–±–∞–≤–ª—è–µ–º —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã: MACD, RSI, Bollinger Bands, 2 HMA, ATR üìà")

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∫–æ–ª–æ–Ω–∫–∏ 'close'
        if 'close' not in df.columns:
            self.logger.error("–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –∫–æ–ª–æ–Ω–∫–∞ 'close' –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤ ‚ùå")
            raise ValueError("–ö–æ–ª–æ–Ω–∫–∞ 'close' –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–∞ –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤")

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏—Å—Ö–æ–¥–Ω—ã–µ –∏–Ω–¥–µ–∫—Å—ã –¥–ª—è –ø–æ—Å–ª–µ–¥—É—é—â–µ–≥–æ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è
        original_index = df.index

        # –°–±—Ä–∞—Å—ã–≤–∞–µ–º –∏–Ω–¥–µ–∫—Å –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–π —Ä–∞–±–æ—Ç—ã TA-Lib
        df = df.reset_index(drop=True)

        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º MACD (12,26,9)
        self.logger.debug("–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º MACD (12,26,9) üìä")
        macd, macd_signal, macd_hist = talib.MACD(df['close'], fastperiod=12, slowperiod=26, signalperiod=9)
        df['macd'] = macd
        df['macd_signal'] = macd_signal
        df['macd_hist'] = macd_hist

        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º RSI (14)
        self.logger.debug("–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º RSI (14) üìä")
        df['rsi'] = talib.RSI(df['close'], timeperiod=14)

        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º Bollinger Bands (20, 2)
        self.logger.debug("–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º Bollinger Bands (20) üìä")
        upper, middle, lower = talib.BBANDS(df['close'], timeperiod=20)
        df['bb_upper'] = upper
        df['bb_mid'] = middle
        df['bb_lower'] = lower

        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –¥–≤–µ HMA —Å —Ä–∞–∑–Ω—ã–º–∏ –ø–µ—Ä–∏–æ–¥–∞–º–∏ (9 –∏ 21)
        # –ö–∞–∫ —É–∫–∞–∑–∞–Ω–æ –≤ –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö, HMA –Ω–µ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω –Ω–∞–ø—Ä—è–º—É—é –≤ TA-Lib –∏ –¥–æ–ª–∂–µ–Ω —Ä–∞—Å—Å—á–∏—Ç—ã–≤–∞—Ç—å—Å—è —á–µ—Ä–µ–∑ WMA [[6]]
        self.logger.debug("–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º Hull Moving Average (HMA) —Å –ø–µ—Ä–∏–æ–¥–∞–º–∏ 9 –∏ 21 üìê")
        df['hma_fast'] = self.calculate_hma(df['close'], 9)
        df['hma_slow'] = self.calculate_hma(df['close'], 21)


        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º ATR (14)
        self.logger.debug("–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º Average True Range (ATR) —Å –ø–µ—Ä–∏–æ–¥–æ–º 14 üìê")
        atr = talib.ATR(df['high'], df['low'], df['close'], 14)
        df['atr'] = atr

        # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∏—Å—Ö–æ–¥–Ω—ã–µ –∏–Ω–¥–µ–∫—Å—ã
        df.index = original_index
        # –∑–∞–ø–æ–ª–Ω—è–µ–º NaN –Ω—É–ª—è–º–∏
        df.fillna(0, inplace=True)


        # –û–±–Ω–æ–≤–ª—è–µ–º —Å–ø–∏—Å–æ–∫ feature_columns —Å –Ω–æ–≤—ã–º–∏ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–∞–º–∏
        new_features = ['macd', 'macd_signal', 'macd_hist', 'rsi', 'bb_upper', 'bb_mid', 'bb_lower', 'hma_fast', 'hma_slow', 'atr']
        self.feature_columns += new_features

        self.logger.success(f"–£—Å–ø–µ—à–Ω–æ –¥–æ–±–∞–≤–ª–µ–Ω–æ {len(new_features)} —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤! üéØ")
        # –í—ã–≤–æ–¥–∏–º –ø—Ä–µ–≤—å—é –¥–∞–Ω–Ω—ã—Ö
        self.logger.info("üìä –ü—Ä–µ–≤—å—é –¥–∞–Ω–Ω—ã—Ö:")

        self.logger.info(f"\n {tabulate(df.head(5), showindex=True, headers='keys', tablefmt='psql')}")
        self.logger.info(f"\n {tabulate(df.tail(5), showindex=True, headers='keys', tablefmt='psql')}")


        return df

    @logger.catch
    def validate_data(self, df: pd.DataFrame) -> None:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç–∏ –¥–∞–Ω–Ω—ã—Ö —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ–± –æ—à–∏–±–∫–∞—Ö"""
        self.logger.debug("üîç –ù–∞—á–∞–ª–æ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö")

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫
        required_columns = ["date"] + self.feature_columns
        missing = [col for col in required_columns if col not in df.columns]
        if missing:
            self.logger.error(f"‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –∫–æ–ª–æ–Ω–∫–∏: {missing}")
            raise ValueError(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –∫–æ–ª–æ–Ω–∫–∏: {missing}")

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–æ—Ä–º–∞—Ç–∞ –¥–∞—Ç—ã
        try:
            df["date"] = pd.to_datetime(df["date"])
            self.logger.debug("üìÖ –î–∞—Ç–∞ —É—Å–ø–µ—à–Ω–æ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∞ –≤ datetime —Ñ–æ—Ä–º–∞—Ç")
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –¥–∞—Ç—ã: {str(e)}")
            raise ValueError("–ö–æ–ª–æ–Ω–∫–∞ 'date' –∏–º–µ–µ—Ç –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç") from e

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –Ω–∞–ª–∏—á–∏–µ –ø—Ä–æ–ø—É—Å–∫–æ–≤
        null_counts = df[self.feature_columns].isnull().sum()
        total_null = null_counts.sum()

        if total_null > 0:
            self.logger.warning(f"‚ö†Ô∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ {total_null} –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π")
            for col, count in null_counts.items():
                if count > 0:
                    self.logger.debug(f"üîç –ü—Ä–æ–ø—É—Å–∫–∏ –≤ '{col}': {count}")
        else:
            self.logger.success("‚úÖ –ù–µ—Ç –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –≤ –¥–∞–Ω–Ω—ã—Ö")

        self.logger.success("‚úÖ –í–∞–ª–∏–¥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")

    @logger.catch
    def create_windows(self, df: pd.DataFrame, dataset_type: str) -> Tuple[
        List[np.ndarray], Dict[str, Tuple[str, datetime]]]:
        """–°–æ–∑–¥–∞–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫–æ–Ω –∏–∑ –¥–∞–Ω–Ω—ã—Ö"""
        self.logger.info(f"üîÑ –°–æ–∑–¥–∞–Ω–∏–µ –æ–∫–æ–Ω –¥–ª—è {dataset_type} (—Ä–∞–∑–º–µ—Ä –æ–∫–Ω–∞: {self.window_size})")

        windows = []
        keys_map = {}
        total_rows = len(df)

        if total_rows < self.window_size:
            self.logger.error(
                f"‚ùå –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –æ–∫–æ–Ω (—Ç—Ä–µ–±—É–µ—Ç—Å—è {self.window_size}, –¥–æ—Å—Ç—É–ø–Ω–æ {total_rows})")
            return [], {}

        # –ü—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä –¥–ª—è –±–æ–ª—å—à–∏—Ö –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö
        start_time = time.time()
        progress_interval = max(1, (total_rows - self.window_size) // 10)

        for i in range(total_rows - self.window_size + 1):
            if i % progress_interval == 0:
                progress = (i + 1) / (total_rows - self.window_size + 1) * 100
                self.logger.debug(
                    f"‚è≥ –ü—Ä–æ–≥—Ä–µ—Å—Å —Å–æ–∑–¥–∞–Ω–∏—è –æ–∫–æ–Ω: {progress:.1f}% ({i + 1}/{total_rows - self.window_size + 1})")

            window_data = df.iloc[i:i + self.window_size][self.feature_columns].values
            last_date = df.iloc[i + self.window_size - 1]["date"]

            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–∫–Ω–∞ –∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–µ–≥–æ –∫–ª—é—á–∞
            windows.append(window_data)
            keys_map[str(len(windows) - 1)] = (self.ticker, last_date)

        elapsed = time.time() - start_time
        self.logger.success(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ {len(windows)} –æ–∫–æ–Ω –∑–∞ {elapsed:.2f} —Å–µ–∫—É–Ω–¥ üöÄ")
        return windows, keys_map

    @logger.catch
    def save_npz(self, windows: List[np.ndarray], keys_map: Dict, output_path: str) -> None:
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –≤ —Ñ–æ—Ä–º–∞—Ç–µ .npz"""
        self.logger.info(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ {len(windows)} –æ–∫–æ–Ω –≤ {output_path}")

        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞–Ω–Ω—ã—Ö
        arrays = {str(i): window for i, window in enumerate(windows)}
        arrays["_keys_map_"] = keys_map

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–æ–≤
        if windows:
            sample = windows[0]
            self.logger.debug(f"üìê –§–æ—Ä–º–∞—Ç –¥–∞–Ω–Ω—ã—Ö: {sample.shape} (–ø—Ä–∏–º–µ—Ä –¥–ª—è –æ–∫–Ω–∞ 0)")

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å –∏–∑–º–µ—Ä–µ–Ω–∏–µ–º –≤—Ä–µ–º–µ–Ω–∏
        start_time = time.time()
        np.savez_compressed(output_path, **arrays)
        elapsed = time.time() - start_time

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–∞ —Ñ–∞–π–ª–∞
        file_size = Path(output_path).stat().st_size / (1024 * 1024)
        self.logger.success(f"‚úÖ –§–∞–π–ª —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {output_path} ({file_size:.2f} MB, {elapsed:.2f} —Å–µ–∫) üì¶")

    @logger.catch
    def process_dataset(self, csv_path: str, output_files: Dict[str, str], percentages: Dict[str, float]) -> None:
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö"""
        self.logger.info(f"üöÄ –ù–∞—á–∞–ª–æ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö –∏–∑ {csv_path}")
        start_time = time.time()

        try:
            # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
            self.logger.info("üì• –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ CSV")
            df = pd.read_csv(csv_path)
            self.logger.info(f"üìä –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} –∑–∞–ø–∏—Å–µ–π")

            # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤
            df = self.add_technical_indicators(df)

            # –í–∞–ª–∏–¥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
            self.validate_data(df)

            # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –∏ —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞
            self.logger.info("üîÑ –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –∏ —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö")
            df["date"] = pd.to_datetime(df["date"])
            df = df.sort_values("date").reset_index(drop=True)
            self.logger.info(f"üìÖ –û—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–æ {len(df)} –∑–∞–ø–∏—Å–µ–π —Å {df['date'].min()} –ø–æ {df['date'].max()}")

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è
            time_diff = df["date"].diff().min()
            self.logger.debug(f"‚è±Ô∏è –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –∏–Ω—Ç–µ—Ä–≤–∞–ª –¥–∞–Ω–Ω—ã—Ö: {time_diff}")

            # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
            self.logger.info("‚úÇÔ∏è –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –Ω–∞ –Ω–∞–±–æ—Ä—ã")
            total = len(df)
            cumulative = 0
            splits = {}

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—É–º–º—ã –ø—Ä–æ—Ü–µ–Ω—Ç–æ–≤
            total_percent = sum(percentages.values())
            if abs(total_percent - 1.0) > 0.01:
                self.logger.warning(f"‚ö†Ô∏è –°—É–º–º–∞ –ø—Ä–æ—Ü–µ–Ω—Ç–æ–≤ ({total_percent:.2f}) –Ω–µ —Ä–∞–≤–Ω–∞ 1.0, –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∞")
                percentages = {k: v / total_percent for k, v in percentages.items()}

            for dataset, percent in percentages.items():
                if percent <= 0:
                    continue

                segment_size = int(total * percent)
                end_idx = min(cumulative + segment_size, total)
                splits[dataset] = (cumulative, end_idx)
                cumulative = end_idx
                self.logger.info(f"üéØ {dataset}: {percent:.1%} ({segment_size} –∑–∞–ø–∏—Å–µ–π) [{cumulative}/{total}]")

            # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞–∂–¥–æ–≥–æ –Ω–∞–±–æ—Ä–∞
            for dataset, (start_idx, end_idx) in splits.items():
                if dataset not in output_files:
                    continue

                self.logger.info(f"üîß –û–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–∞–±–æ—Ä–∞ '{dataset}'")
                segment = df.iloc[start_idx:end_idx].copy()

                # –°–æ–∑–¥–∞–Ω–∏–µ –æ–∫–æ–Ω
                windows, keys_map = self.create_windows(segment, dataset)

                # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
                if windows:
                    self.save_npz(windows, keys_map, output_files[dataset])
                else:
                    self.logger.warning(f"‚ö†Ô∏è –ü—Ä–æ–ø—É—Å–∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è {dataset}: –Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–∫–æ–Ω")

            total_time = time.time() - start_time
            self.logger.success(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ –∑–∞ {total_time:.2f} —Å–µ–∫—É–Ω–¥! üéØ")

        except Exception as e:
            self.logger.exception(f"üí• –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –¥–∞–Ω–Ω—ã—Ö: {str(e)}")
            raise


# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
if __name__ == "__main__":
    logger.info("üöÄ –ó–∞–ø—É—Å–∫ Financial Data Preprocessor")

    try:
        # –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–µ –ø—Ä–æ—Ü–µ–Ω—Ç—ã (—Ä–∞–Ω–µ–µ –±—ã–ª–æ 0.5 –¥–ª—è val, —á—Ç–æ –¥–∞–≤–∞–ª–æ 145%)
        percentages = {
            "train": 0.75,  # 75% –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
            "val": 0.05,  # 5% –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏ (–±—ã–ª–æ 0.5 - –æ—à–∏–±–∫–∞)
            "test": 0.10,  # 10% –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
            "backtest": 0.10  # 10% –¥–ª—è –±—ç–∫—Ç–µ—Å—Ç–∞
        }

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—É–º–º—ã –ø—Ä–æ—Ü–µ–Ω—Ç–æ–≤
        total_percent = sum(percentages.values())
        if abs(total_percent - 1.0) > 0.01:
            logger.warning(f"‚ö†Ô∏è –°—É–º–º–∞ –ø—Ä–æ—Ü–µ–Ω—Ç–æ–≤ ({total_percent:.2f}) –Ω–µ —Ä–∞–≤–Ω–∞ 1.0")

        preprocessor = FinancialDataPreprocessor(
            ticker="DOGEUSDT",
            window_size=150
        )

        preprocessor.process_dataset(
            csv_path="data.csv",
            output_files={
                "train": "train_data.npz",
                "val": "val_data.npz",
                "test": "test_data.npz",
                "backtest": "backtest_data.npz"
            },
            percentages=percentages
        )

        logger.success("üéâ –í—Å–µ —Ñ–∞–π–ª—ã —É—Å–ø–µ—à–Ω–æ —Å–æ–∑–¥–∞–Ω—ã —Å —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–º–∏ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–∞–º–∏!")

    except Exception as e:
        logger.critical(f"üí• –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã –∏–∑-–∑–∞ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–æ–π –æ—à–∏–±–∫–∏: {str(e)}")
        sys.exit(1)